---
cluster_name: hadoop-poc
hdp_version: '2.4'
java_type: oracle
custom_repo: false
custom_repo_url: ''
custom_repo_target: 'api/v1/stacks/HDP/versions/2.4/operating_systems/redhat6/repositories/HDP-2.4'
ambari_version: '2.2.1.1'
install_spark: true
install_flume: true
install_hbase: false
install_storm: true
install_kafka: true
install_falcon: true
tachyon_service: false
custom_blueprint: false
admin_password: 'admin'
services_password: 'AsdQwe123'
alerts_contact: 'root@localhost.localdomain'
data_disks_filesystem: xfs
configure_firewall: false
update_hosts_file: true

hadoop:
  create_cluster: true  ## Trigger 'Cluster Creation' in Ambari Server

spark_stack: false
spark_stack_config:
  tachyon_worker_memory: '1GB'
  spark_exector_memory: '1g'
  spark_driver_memory: '1g'
  spark_driver_maxResultSize: '512M'
  zeppelin_spark_executor_memory: '1g'
  zeppelin_spark_cores_max: '1'
  default_user: 'dgrier'

hdfs:
  dfs_replication: 3
  failed_volumes_tolerated: 1
  ha_namenode: true

cloud_config:
  rax_credentials_file: '~/.raxpub'
  rax_region: 'ORD'

  domain: 'localnet'
  allowed_external_ips: ['127.0.0.1']
  ssh:
    keyname: 'hadoop-ssh-key'
    keyfile: '~/.ssh/id_rsa.pub'

# set to true to show host variables
debug: false
